Post Title: Genesis-152M-Instruct â€” Hybrid GLA + FoX + Test-Time Training at small scale
Author: Kassanar
Score: 2
URL: https://www.reddit.com/r/huggingface/comments/1pwa0ms/genesis152minstruct_hybrid_gla_fox_testtime/
Number of comments: 0
Created UTC: 1766770705.0

Post Content:
Hey everyone ğŸ‘‹

Iâ€™m sharing **Genesis-152M-Instruct**, an **experimental small language model** built to explore how *recent architectural ideas interact* when combined in a single model â€” especially under **tight data constraints**.



This is **research-oriented**, not a production model or SOTA claim.





ğŸ” **Why this might be interesting**



Most recent architectures (GLA, FoX, TTT, ÂµP, sparsity) are tested **in isolation** and usually at **large scale**.

I wanted to answer a simpler question:



*How much can architecture compensate for data at \~150M parameters?*



Genesis combines several **ICLR 2024â€“2025 ideas** into one model and evaluates the result.





âš¡ **TL;DR**

â€¢ **152M parameters**

â€¢ Trained on **\~2B tokens** (vs \~2T for SmolLM2)

â€¢ Hybrid **GLA + FoX attention**

â€¢ **Test-Time Training (TTT)** during inference

â€¢ **Selective Activation (sparse FFN)**

â€¢ **ÂµP-scaled training**

â€¢ Fully open-source (Apache 2.0)



ğŸ¤— Model: [https://huggingface.co/guiferrarib/genesis-152m-instruct](https://huggingface.co/guiferrarib/genesis-152m-instruct)

ğŸ“¦ pip install genesis-llm





ğŸ“Š **Benchmarks (LightEval, Apple MPS)**



ARC-Easy Â  Â  â†’ 44.0% Â  (random: 25%)

BoolQÂ  Â  Â  Â  â†’ 56.3% Â  (random: 50%)

HellaSwagÂ  Â  â†’ 30.2% Â  (random: 25%)

SciQ Â  Â  Â  Â  â†’ 46.8% Â  (random: 25%)

Winogrande Â  â†’ 49.1% Â  (random: 50%)



**Important context:**

SmolLM2-135M was trained on **\~2 trillion tokens**.

Genesis uses **\~2 billion tokens** â€” so this is not a fair head-to-head, but an exploration of **architecture vs data scaling**.





ğŸ§  **Architecture Overview**



**Hybrid Attention (Qwen3-Next inspired)**



**Layer** **%** **Complexity** **Role**

Gated DeltaNet (GLA) 75% O(n) Long-range efficiency

FoX (Forgetting Attention) 25% O(nÂ²) Precise retrieval



GLA uses:

â€¢ Delta rule memory updates

â€¢ Mamba-style gating

â€¢ L2-normalized Q/K

â€¢ Short convolutions



FoX adds:

â€¢ Softmax attention

â€¢ Data-dependent forget gate

â€¢ Output gating





**Test-Time Training (TTT)**



Instead of frozen inference, Genesis can **adapt online**:

â€¢ Dual-form TTT (parallel gradients)

â€¢ Low-rank updates (rank=4)

â€¢ Learnable inner learning rate



Paper: *Learning to (Learn at Test Time)* (MIT, ICML 2024)





**Selective Activation (Sparse FFN)**



SwiGLU FFNs with **top-k activation masking** (85% kept).

Currently acts as **regularization** â€” real speedups need sparse kernels.





**ÂµP Scaling + Zero-Centered RMSNorm**

â€¢ Hyperparameters tuned on small proxy

â€¢ Transferred via ÂµP rules

â€¢ Zero-centered RMSNorm for stable scaling





âš ï¸ **Limitations (honest)**

â€¢ Small training corpus (2B tokens)

â€¢ TTT adds \~5â€“10% inference overhead

â€¢ No RLHF

â€¢ Experimental, not production-ready





ğŸ“ **Links**

â€¢ ğŸ¤— Model: [https://huggingface.co/guiferrarib/genesis-152m-instruct](https://huggingface.co/guiferrarib/genesis-152m-instruct)

â€¢ ğŸ“¦ PyPI: [https://pypi.org/project/genesis-llm/](https://pypi.org/project/genesis-llm/)





Iâ€™d really appreciate feedback â€” especially from folks working on **linear attention**, **hybrid architectures**, or **test-time adaptation**.



*Built by Orch-Mind Team*


Top 0 comments:
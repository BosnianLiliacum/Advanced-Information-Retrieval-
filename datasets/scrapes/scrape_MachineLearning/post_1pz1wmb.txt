Post Title: [R] End-to-End Test-Time Training for Long Context
Author: karansdalal
Score: 26
URL: https://www.reddit.com/r/MachineLearning/comments/1pz1wmb/r_endtoend_testtime_training_for_long_context/
Number of comments: 1
Created UTC: 1767051318.0

Post Content:
[https://test-time-training.github.io/e2e.pdf](https://test-time-training.github.io/e2e.pdf)

We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture – a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model’s initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7× faster than full attention for 128K context. Our code is publicly available.


Top 1 comments:

Comment 1:
Author: radarsat1
Score: 3
Created UTC: 1767089410.0
Comment: This was an interesting read. The section on fast weights at the end made me wonder if there could additionally be some connection to speculative decoding. Imagine instead of updating some subset of layers on the test sample, we update a smaller model and use the larger model with full attention just to verify, could we get the accuracy of the large model back on tasks like needle-haystack? with hopefully minimal efficiency penalties..
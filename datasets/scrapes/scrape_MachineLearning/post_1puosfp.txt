Post Title: [P] SIID: A scale invariant pixel-space diffusion model; trained on 64x64 MNIST, generates readable 1024x1024 digits for arbitrary ratios with minimal deformities (25M parameters)
Author: Tripel_Meow
Score: 40
URL: https://www.reddit.com/r/MachineLearning/comments/1puosfp/p_siid_a_scale_invariant_pixelspace_diffusion/
Number of comments: 11
Created UTC: 1766587019.0

Post Content:
GitHub repository: [https://github.com/Yegor-men/scale-invariant-image-diffuser](https://github.com/Yegor-men/scale-invariant-image-diffuser)

^(Sorry in advance for the not-so-clean training and inference code in the repository, as well as the .pt and not .safetensors modelfiles. I understand the concerns, and will update the code soon. I simply wanted to share/showcase the progress thus far. The code for the actual model architecture will not be changed, so that's the main purpose of the post. Detailed explanation of the architecture is at the end of the post.)

Hello everyone,

Over the past couple weeks/months I've been working on my own diffusion architecture which aims to solve a couple of key gripes I have with UNet/DiT diffusion architectures. Namely:

* UNet heavily relies on convolution kernels, and convolution kernels are trained to a certain pixel density. Change the pixel density (by increasing the resolution of the image via upscaling) and your feature detector can no longer detect those same features, which is why you get these doubling artifacts when you increase the resolution on SDXL models for example.
* DiT uses RoPE, which in itself is not bad, but adding more pixels makes it so that the newly-added pixels get entirely new positional embeddings. This makes sense in LLMs, as each token is already atomic, but this makes little sense for pictures where you can infinitely subdivide a pixel. If you upscale an image by 2x, 3/4 of the positional embeddings for the pixel are completely new. It's like having an LLM trained on one context length, and then all of a sudden requesting it to do double that, or maybe even more. Not really reliable.

So instead, I set out to make my own architecture, with the key idea being that **adding more pixels doesn't add more information, it simply refines it**. My point being, pixel density should not affect the quality of the diffusion process. So, after some months of work, I made **SIID (Scale Invariant Image Diffuser)**. In short (much more detailed explanation later), SIID primarily relies on the following (simplified) workflow:

* (Optional but recommended) The model first compresses the height and width of the image into more channels via pixel unshuffle. No information about the image is lost, it's simply moved to the channels to decrease the "token" count and increase speed.
* Two separate types of relative positional embedding allow the model to understand where the pixel is relative to the composition and where the pixel is relative to the actual image; this allows the model to understand where the image edges are while also not forming the entire composition based on that (for aspect ratios outside the trained, the second positional conditioning system will yield "new" coordinates; more detailed explanation later).
* The number of channels is expanded from the base number (color channels + position channels) out into much more, akin to how tokens in LLMs are larger than necessary: it's so that each token can hold the information about the context.
* "Encoder" transformer blocks based on axial attention allow the model to first understand the composition of the image, and also suggests at image editing capabilities like FLUX Kontext. A learnable gaussian distribution masking helps the model to focus on spatially close features first (the distribution is in relative distance, such as 3 standard deviations would cover the full image width assuming it were a square; more detailed explanation later).
* "Decoder" transformer blocks based on axial attention and also utilizing cross attention for the text conditioning allow the model to now understand the spatial features, composition, et cetera. Since the encoder blocks don't use text conditioning, the decoder blocks re-use the output of the encoder for each of the conditionings (null, positive, negative), meaning that one forward pass is more efficient.
* The fully attended "latent" is now turned back into pixel space, and thus is the predicted epsilon noise.

So, I made SIID to train **exclusively** on 64x64 (bicubic upscaled), **unaugmented** MNIST images. I used 8 encoder blocks and 8 decoder blocks. The rescale factor is 8, meaning that the model was trained on what is effectively an 8x8 image. Each of these latent pixels has 256 channels (64 for the color after the pixel unshuffle, 40 for the positioning system; leaves 152 channels for the model to attend extra info around and about). All this combined results in a model just shy of **25M parameters**. Not bad considering that it can actually diffuse images at 1024x1024 such that the digits are still readable:

[Trained on 64x64, diffused at 1024x1024](https://preview.redd.it/tnwkl5cl759g1.png?width=800&amp;format=png&amp;auto=webp&amp;s=c55120882c9a8e09f4d8c4d7624a501767045683)

The digits are blurry, yes, but the fact is that for **99.61%** of the pixels, the model has never seen those coordinates before, and yet it can still produce readable digits. The model was trained on coordinates for an 8x8 latent, and yet scales quite well to a 128x128 latent. This seems to imply that the model architecture can scale **very** well with size, especially when we consider what the digits look like at more "native" resolutions, closer to that 8x8 latent.

Such as the default 64x64 resolution that the model was trained on (keep in mind that for this, and all the following diffusion results, 100 ddim steps were used, cfg of 4.0, eta of 2.0):

[1:1 aspect ratio, 64x64, the native resolution that SIID was trained on](https://preview.redd.it/1efixo11f59g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=d0e48064b0cdbd606717d5a18917f9cb160bc451)

Now remember that SIID was trained **exclusively** on 64x64 images with no augmentations, now let's take a look at the results for images with an aspect ratio outside the trained 64x64 (8x8 latent):

[2:3 aspect ratio, 72x48, resulting in a 9x6 latent](https://preview.redd.it/c2401qhwf59g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=269ef86dc24a59cf2878908d2629e4487eba5b2b)

[3:2 aspect ratio, 48x72 image, resulting in a 6x9 latent](https://preview.redd.it/tmevdqhwf59g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=d1635473e6b111f3fbe05005d1c5096ebf29599c)

As you can see, the model still largely diffuses quite fine, all the digits are legible. However, it must be pointed out that with the way the positioning system works, most of the coordinates here are **actually novel**, due to the fact that these sizes don't nicely align with the trained resolution, but more importantly due to the second kind of positioning system that SIID uses (more detailed explanation later). What's interesting to note is that in spite of this, SIID dynamically adjusts the digits to make them fit (again, no data augmentation used for training). When the image is vertical, SIID simply crops out the black space. When the image is horizontal, SIID compresses the digit a bit to make it fit.

Let's take a look at some other aspect ratios, namely 3:4, 4:5 and even 9:16 to really test the limits. This is going to result in latent sizes of 6x8, 8x10 and 9x16 respectively. In any case, let's take a look:

[3:4 aspect ratio, 64x48 image, resulting in an 8x6 latent](https://preview.redd.it/o3hj44emh59g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=ebd2664418407966e4c369c08179d13033305862)

[4:3 aspect ratio, 48x64 image, resulting in a 6x8 latent](https://preview.redd.it/sfafq4emh59g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=696f0994d1db4df6074a14288f959ae997f8a3d0)

[4:5 aspect ratio, 80x64 image, resulting in a 10x8 latent](https://preview.redd.it/nop1p4emh59g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=cbbb4d8376d84fb57da5f0fcbd51e8ef0564e8ca)

[5:4 aspect ratio, 64x80 image, resulting in a 8x10 latent](https://preview.redd.it/0si224emh59g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=b0b06b6f98339233a9674d431002ed6622d430b9)

[9:16 aspect ratio, 128x72 image, resulting in a 16x9 latent](https://preview.redd.it/zwsdxykql59g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=4d5d0270b9b926bd3d36e57305ccd981247c13ba)

[16:9 aspect ratio, 72x128 image, resulting in a 9x16 latent](https://preview.redd.it/uvdhozkql59g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=f72902b4dff473ea5a431ca060210a5b50f07572)

A similar story as with the other aspect ratios, the model diffuses largely fine in spite of the fact that these aren't trained aspect ratios or resolutions. SIID crops out the blank space on the sides when it can, and squishes the digit a bit when it has to. We see artifacts on some of these digits, but this should be easily fixable with the proper image augmentation techniques (resizes and crops), as right now, most of these coordinates are (very crudely) interpolated. We can see how the 16:9 and 9:16 aspect ratios are *really* pushing the limits, but SIID seems to hold up considering everything thus far.

**It's also worth noting that a proper diffusion model will be trained on much larger images, such as 512x512 or 1024x1024, which results in much longer sequences in the latent such as 64x64 or 128x128, which will create significantly cleaner interpolation, so most of these artifacts should (in theory) disappear at those sizes.**

For the sake of completion, let's also quickly look at 128x128 and 256x256 images produced by SIID:

[1:1 aspect ratio, 128x128 image, resulting in a 16x16 latent](https://preview.redd.it/8efhnqtrj59g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=06282ac82923492da84ff44218167a38a887381a)

[1:1 aspect ratio, 256x256 image, resulting in a 32x32 latent](https://preview.redd.it/69e549wrj59g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=103ee1048a0a9e55a4547afb2a21624543cdea3a)

As you can see here, we get these kind of ripple artifacts that we don't see before. This is very most likely due to the fact that 3/4 the coordinates are interpolated for the 128x128 image, and 15/16 of the coordinates are interpolated for the 256x256 image. While arguably uglier than the 1024x1024 image, the results look just as promising: again, considering the fact that a sequence length of 8 "tokens" is really short, and also considering that the model wasn't trained on image augmentations.

So, there's that. SIID was trained on unaugmented 64x64 images, which results in an 8x8 latent, and yet the model seems promising to use for drastically varying aspect ratios and resolutions. The further we stray from the base trained resolution, the more artifacts we experience, but at the same time, the composition doesn't change, suggesting that we can rid ourselves of the artifacts with proper image augmentation. When we change the aspect ratio, the digits don't get cropped, only squished when necessary, although this was never in the training data. This seems to suggest the dual relative positioning system works just as intended: the model both understands the concept of the composition (what the underlying function is), as well as the actual image restrictions (a view of the composition).

(Edit) Here's the t scrape loss, the MSE loss that SIID gets over t (the thing that goes into the alpha bar function), for null and positive conditioning. SIID was trained for 72,000 AdamW optimizer steps with a cosine scheduler with the LR going from 1e-3 down to 1e-5, 1,200 warmup steps. I'd want the model to require less cfg and less noise in order to work, but I assume that I need to fix my learning rate scheduling for that as maybe 1e-5 is too big or something? Don't know.

[t scrape MSE loss](https://preview.redd.it/vumkzhmj369g1.png?width=640&amp;format=png&amp;auto=webp&amp;s=41d1c3be6e3f35a004ae063a0e1b848b37bd0a74)

So that's it for the showcase. Now for the much more detailed explanations of how the architecture works. The full code is available on the repository, this here is simply an explanation of what is going on:

* FiLM (AdaLN) time conditioning is heavily used throughout SIID, in both the "encoder" and "decoder" transformer blocks: before the axial attention, before the cross attention, and before the FNN equivalent. The vector for FiLM is produced at the start from the alpha bar (value between 0 and 1 representing how corrupted the image is) which is a smooth fourier series passed though an MLP with SiLU, nothing special.
* Residual and skip connections are used in the blocks and between the blocks.
* The "relative positioning system" mentioned earlier is actually comprised of two parts ( both are relative but are named "relative" and "absolute" for the sake of how they work in the relative space). The key feature of both of these systems is that they use a modified RoPE with increasing frequencies, not decreasing. For long range context such as in LLMs, lower and lower frequencies are used, such that the wavelengths can cover more and more tokens; you easily have wavelengths that cover tens of thousands of tokens. For SIID, the frequencies are **increasing** instead, because as said before, the pixels can be infinitely subdivided; we need higher and higher frequencies to distinguish them, while the lowest of frequencies would span multiple images (if there was the space for it, which there isn't). Point being, for the case of SIID on 64x64 MNIST, the frequencies used were \[pi/8, pi/4, pi/2, pi, 2pi\] which were made to span the image height/width. The rest of the RoPE approach (sin/cos, exponential frequencies) is the same as usual.
   * The first system which is called "relative" works as follows: When comes the time to assign coordinates to the latent pixels (latent pixels simply being the unshuffled image to compress the height and width into the color channels), it takes the latent image and inscribes it into a square. So a 16x9 latent is inscribed into a 16x16 square, and centered. Next, on that square, the edges are assigned to be +-0.5 respectfully as a smooth linspace. The coordinates for the actual pixels are taken as to where the pixels of the image are on that square, meaning that the center of the image always gets (0, 0), while the maximum will always ever be (0.5, 0.5) (if the image is a square that is). The point of this system is so that the model understands composition. No matter the aspect ratio (crop) of the image, the underlying subject that the image is trying to depict doesn't change, the subject is created based on this relative coordinate system. This is good, but if we use only this system and nothing else, then when we train on one aspect ratio, and then change it, the model can easily just crop the digit out (that's what happened in early training). Thus we also create a second system to balance it out.
   * The second system which is called "absolute", works similar to the first system, except that we don't inscribe the latent image into a square, we just directly use linspace from -0.5 to 0.5 along the image height and width. The idea here is that the model will now know how far each pixel is to the edges. Now just as before, if we only used this system, and nothing else, then when we train on one aspect ratio and then change it for the diffusion, the digit won't be cropped out, but it will be squished, which is not good as our aspect ratio (crop) is simply a view of the underlying function. Thus we use this "absolute" approach in conjunction with the "relative" approach from before such that each pixel now knows how far it is from the edge of the image, and where it is in the actual composition. With the whole system being based around 0.5 being the edge of the image/edge of the square it's inscribed into, even if we double, triple, or even multiply the resolution of the image by 64 as with the 1024x1024 image example, we don't actually get brand new unseen coordinates that we would have gotten, we simply get lots of interpolated coordinates. When before I mentioned that for different aspect ratios the coordinates are "new", what I meant was that the first coordinate system and second coordinate system work against each other in those examples (since for training on 1:1, the coordinates would have been identical for both systems as a square inscribed in a square is no different, but the instant we change the aspect ratio, one coordinate system stays the same, while the other starts giving "contradictory" signals, and yet it still works).
* The gaussian mask in the "encoder" transformer blocks has a learnable \`sigma\` (standard deviation), which isn't applied directly on the number of pixels there are, but it works in the same way as the "relative" coordinate system works, in that the sigma dictates how far for context relative to the composition the attention should pass along information. Point being, a sigma of 0.1667 would imply that 3 standard deviations is 0.5, thus covering the entire image; a pixel in the middle of the image would thus attend to all other pixels in the image with an accordingly decreasing rate (a pixel on the edge would hence attend to the other ones near the edge), regardless of the actual size of the latent image. The reason that this approach is used in the first place is to help the "encoder" transformer blocks make up for the lack of the convolutions. SIID already covers locations/positioning in the KQV for attention, but this extra mask is meant specifically to function as the local feature capturer.
* The reason that the pixel unshuffle and pixel shuffle is used is explicitly for speed, nothing more. In earlier tests I did it in raw pixel space, and it was too slow for my liking as the model needed to do attentions on sequence length of 28 and not 8 (which becomes even slower considering the fact that the \[B, D, H, W\] tensor is reshaped to multiply the batch size by the width/height to turn it into the effective batch size for the axial attention, a reduction from 28 to 8 is massive as it's both a shorter sequence and a smaller batch size). It's certainly doable, and this is what will have to be done for a proper model, but it was too slow for a dummy task. However, the important part here being that SIID is a diffusion model only, **you could very well and easily use it in conjunction with a VAE**, meaning that you could speed it up even more if you wanted to by making SIID predict latent noise instead.

In any case, I think that's it? I can't think of anything else to say. All the code can be found in the [repository](https://github.com/Yegor-men/scale-invariant-image-diffuser) mentioned above. Yet again, forgive for the unclean training and inference code, as well as the .pt and not .safetensors models to test the models. I am aware of the concerns/risks, and I will update the code in the future. However, the architecture is set in stone, I don't think I'll change it, at least I don't have any meaningful ideas on how to change it. Thus I'm open to critique, suggestions and questions.

Kind regards,


Top 5 comments:

Comment 1:
Author: BigMrWeeb
Score: 13
Created UTC: 1766591103.0
Comment: If you're interested in this sort of stuff, it might be worth looking into some of literature on functional/infinite dimensional diffusion? - diffusing in function spaces and sampling at arbitrary densities.

Its been a while, but I believe the premise behind a lot of them is to instead operate on the fourier space of an image, then fix local issues using 1d convs after doing some padding to the chosen dimension (this is obviously a bit of a simplification). The math is a bit heavy, but its often overkill for what they actually end up doing, and by looking through their code bases you might find some inspiration?

Comment 2:
Author: TachyonGun
Score: 8
Created UTC: 1766645370.0
Comment: I don't have much to add, I just want to say this was a good write up, it shows your technical acumen and I learned a few things. I will probably revisit the repo later. Your post raises the bar for the sub and was a solace from the walls of slop from other "practitioners", thank you for taking the time to write this up!

Comment 3:
Author: 1deasEMW
Score: 2
Created UTC: 1766655924.0
Comment: Interesting stuff a bit dense, will come back 
!remindme

Comment 4:
Author: jdude_
Score: 1
Created UTC: 1766648428.0
Comment: Just curious, wouldn't scaling cosine positional embeddings the same way yield similar results?

Comment 5:
Author: Tripel_Meow
Score: 1
Created UTC: 1766778756.0
Comment: UPDATE: This post is now old, the architecture was changed quite a bit. See the new post [here](https://www.reddit.com/r/MachineLearning/comments/1pwd8su/p_s2id_scale_invariant_image_diffuser_trained_on/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button)
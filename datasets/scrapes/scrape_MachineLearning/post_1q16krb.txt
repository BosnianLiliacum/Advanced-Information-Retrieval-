Post Title: [P] I built a drop-in Scikit-Learn replacement for SVD/PCA that automatically selects the optimal rank (Gavish-Donoho)
Author: Single_Recover_8036
Score: 14
URL: https://www.reddit.com/r/MachineLearning/comments/1q16krb/p_i_built_a_dropin_scikitlearn_replacement_for/
Number of comments: 0
Created UTC: 1767280887.0

Post Content:
Hi everyone,

I've been working on a library called `randomized-svd` to address a couple of pain points I found with standard implementations of SVD and PCA in Python.

**The Main Features:**

1. **Auto-Rank Selection:** Instead of cross-validating `n_components`, I implemented the **Gavish-Donoho hard thresholding**. It analyzes the singular value spectrum and cuts off the noise tail automatically.
2. **Virtual Centering:** It allows performing PCA (which requires centering) on **Sparse Matrices** without densifying them. It computes (X−μ)v implicitly, saving huge amounts of RAM.
3. **Sklearn API:** It passes all `check_estimator` tests and works in Pipelines.

**Why I made this:** I wanted a way to denoise images and reduce features without running expensive GridSearches.

**Example:**

    from randomized_svd import RandomizedSVD
    # Finds the best rank automatically in one pass
    rsvd = RandomizedSVD(n_components=100, rank_selection='auto')
    X_reduced = rsvd.fit_transform(X)

I'd love some feedback on the implementation or suggestions for improvements!

Repo: [https://github.com/massimofedrigo/randomized-svd](https://github.com/massimofedrigo/randomized-svd)

Docs: [https://massimofedrigo.com/thesis\_eng.pdf](https://massimofedrigo.com/thesis_eng.pdf)


Top 0 comments:
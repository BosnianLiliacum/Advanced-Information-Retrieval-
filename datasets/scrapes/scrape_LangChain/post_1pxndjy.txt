Post Title: Advanced RAG: Token Optimization and Cost Reduction in Production. We Cut Query Costs by 60%
Author: Electrical-Signal858
Score: 51
URL: https://www.reddit.com/r/LangChain/comments/1pxndjy/advanced_rag_token_optimization_and_cost/
Number of comments: 13
Created UTC: 1766914411.0

Post Content:
Following up on my previous RAG post: we've optimized production RAG systems further and discovered cost optimizations that nobody talks about. This is specifically about reducing token spend without sacrificing quality.

# The Problem We Solved

Our RAG system was working well (retrieval was solid, generation was accurate), but the token spend kept climbing:

* Hybrid retrieval (BM25 + vector): \~2,000 tokens/query
* Retrieved documents: \~3,000 tokens
* LLM processing: \~500 tokens
* **Total: \~5,500 tokens/query** × 100k queries/day = expensive

At $0.03 per 1K input tokens, that's **$16.50/day just for input tokens**. **$495/month**.

We asked: "Can we get similar quality with fewer tokens?"

Spoiler: Yes. We reduced it to **2,200 tokens/query average** (60% reduction) while maintaining 92% accuracy (same as before).

# The Optimizations

# 1. Smart Document Chunking Reduces Retrieved Token Count

**Before:** Fixed 1,000-token chunks

* Simple but wasteful
* Lots of redundant context
* Padding with irrelevant info

**After:** Semantic chunks with metadata filtering

    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from sentence_transformers import SentenceTransformer
    import numpy as np
    
    class SemanticChunker:
        def __init__(self, min_chunk_size=200, max_chunk_size=800):
            self.min_chunk_size = min_chunk_size
            self.max_chunk_size = max_chunk_size
            self.model = SentenceTransformer('all-MiniLM-L6-v2')
        
        def chunk_semantically(self, text, title=""):
            """Break text into semantic chunks"""
            sentences = text.split('. ')
            
            embeddings = self.model.encode(sentences)
            chunks = []
            current_chunk = []
            current_embedding = None
            
            for i, sentence in enumerate(sentences):
                current_chunk.append(sentence)
                
                if len(' '.join(current_chunk)) &gt;= self.min_chunk_size:
                    # Check semantic coherence
                    chunk_embedding = self.model.encode(' '.join(current_chunk))
                    
                    if current_embedding is not None:
                        # Cosine similarity with previous chunk
                        similarity = np.dot(chunk_embedding, current_embedding) / (
                            np.linalg.norm(chunk_embedding) * np.linalg.norm(current_embedding)
                        )
                        
                        # If semantic break detected or max size reached
                        if similarity &lt; 0.6 or len(' '.join(current_chunk)) &gt;= self.max_chunk_size:
                            chunks.append({
                                'content': ' '.join(current_chunk),
                                'title': title,
                                'tokens': len(' '.join(current_chunk).split())
                            })
                            current_chunk = []
                            current_embedding = None
                            continue
                    
                    current_embedding = chunk_embedding
            
            if current_chunk:
                chunks.append({
                    'content': ' '.join(current_chunk),
                    'title': title,
                    'tokens': len(' '.join(current_chunk).split())
                })
            
            return chunks
    

**Result:** Average chunk size went from 1,000 tokens → 400 tokens (but more relevant). Retrieved fewer chunks but with less padding.

# 2. Retrieval Pre-filtering Reduces What Gets Retrieved

**Before:** "Get top-5 by relevance, send all to LLM"

**After:** Multi-stage retrieval pre-filtering

    def filtered_retrieval(query: str, documents: List[str], top_k=5):
        """Retrieve with automatic filtering"""
        
        # Stage 1: Broad retrieval (get more candidates)
        candidates = vector_store.search(query, top_k=20)
        
        # Stage 2: Filter by relevance threshold
        scored = [(doc, score) for doc, score in candidates]
        high_confidence = [
            (doc, score) for doc, score in scored 
            if score &gt; 0.7  # Only confident matches
        ]
        
        if not high_confidence:
            high_confidence = scored[:5]  # Fallback to top-5
        
        # Stage 3: Deduplicate similar content
        unique = []
        seen_hashes = set()
        
        for doc, score in high_confidence:
            doc_hash = hash(doc[:200])  # Hash of first 200 chars
            
            if doc_hash not in seen_hashes:
                unique.append((doc, score))
                seen_hashes.add(doc_hash)
        
        # Stage 4: Sort by relevance and return top-k
        final = sorted(unique, key=lambda x: x[1], reverse=True)[:top_k]
        
        return [doc for doc, _ in final]
    

**Result:** Retrieved fewer documents, but only high-confidence ones. Reduced retrieved token count by 40%.

# 3. Query Simplification Before Retrieval

**Before:** Send raw user query to retriever

    User: "What are the refund policies for digital products if the customer received 
           a defective item and wants to know about international shipping costs?"
    (Complex, confusing retriever)
    

**After:** Pre-process query to find key concepts

    from langchain.chains import LLMChain
    from langchain.prompts import PromptTemplate
    
    def simplify_query(query: str, llm) -&gt; str:
        """Simplify query for better retrieval"""
        
        prompt = PromptTemplate(
            input_variables=["query"],
            template="""Extract the main topic from this query. 
            Remove adjectives, clarifications, and side questions.
            
            User query: {query}
            
            Simplified: """
        )
        
        chain = LLMChain(llm=llm, prompt=prompt)
        
        # Use cheaper model for this (gpt-3.5-turbo)
        simplified = chain.run(query=query).strip()
        
        return simplified
    
    # Usage:
    simplified = simplify_query(
        "What are the refund policies for digital products if the customer received "
        "a defective item and wants to know about international shipping costs?",
        llm
    )
    # Result: "refund policy digital products"
    

**Result:** Better retrieval queries → fewer iterations → fewer tokens.

# 4. Response Compression Before Sending to LLM

**Before:** Send all retrieved documents as-is

    Retrieved documents (all 3,000 tokens):
    [Document 1: 1000 tokens]
    [Document 2: 1000 tokens]
    [Document 3: 1000 tokens]
    

**After:** Compress while preserving information

    def compress_context(documents: List[str], query: str, llm) -&gt; str:
        """Compress documents while preserving relevant info"""
        
        compression_prompt = PromptTemplate(
            input_variables=["documents", "query"],
            template="""Summarize the following documents in as few words as possible 
            while preserving information relevant to the question.
            
            Question: {query}
            
            Documents:
            {documents}
            
            Compressed summary:"""
        )
        
        chain = LLMChain(llm=llm, prompt=compression_prompt)
        
        documents_text = "\n---\n".join(documents)
        
        compressed = chain.run(
            documents=documents_text,
            query=query
        )
        
        return compressed
    
    # Usage:
    context = compress_context(retrieved_docs, user_query, llm)
    # 3000 tokens → 800 tokens (still has all relevant info)
    

**Result:** 60-70% context reduction with minimal quality loss.

# 5. Caching at the Context Level (Not Just Response Level)

**Before:** Cache full responses only

    cache_key = hash(f"{query}_{user_id}")
    cached_response = cache.get(cache_key)  # Only hits if identical query
    

**After:** Cache compressed context

    def cached_context_retrieval(query: str, user_context: str) -&gt; str:
        """Retrieve and cache at context level"""
        
        # Hash just the query (not user context)
        context_key = f"context:{hash(query)}"
        
        # Check if we've retrieved this query before
        cached_context = cache.get(context_key)
        
        if cached_context:
            return cached_context  # Reuse compressed context
        
        # If not cached, retrieve and compress
        documents = retriever.get_relevant_documents(query)
        compressed = compress_context(documents, query, llm)
        
        # Cache the compressed context
        cache.set(context_key, compressed, ttl=86400)  # 24 hours
        
        return compressed
    
    # Usage:
    context = cached_context_retrieval(query, user_context)
    
    # For identical queries from different users:
    # User A: Retrieves, compresses (3000 tokens), caches
    # User B: Uses cached context (0 tokens)
    

**Result:** Context-level caching hits on 35% of queries (many users asking similar things).

# 6. Token Counting Before Sending to LLM

**Before:** Blindly send context to LLM, hope it fits

    response = llm.generate(system_prompt + context + user_query)
    # Sometimes exceeds context window, sometimes wastes tokens
    

**After:** Count tokens, optimize if needed

    import tiktoken
    
    def smart_context_sending(context: str, query: str, llm, max_tokens=6000):
        """Send context to LLM, optimizing token usage"""
        
        enc = tiktoken.encoding_for_model("gpt-4")
        
        # Count tokens in different parts
        system_tokens = len(enc.encode(SYSTEM_PROMPT))
        query_tokens = len(enc.encode(query))
        context_tokens = len(enc.encode(context))
        
        total_input = system_tokens + query_tokens + context_tokens
        
        # If over budget, compress context further
        if total_input &gt; max_tokens:
            compression_ratio = (total_input - max_tokens) / context_tokens
            
            # Aggressive compression if needed
            compressed = aggressive_compress(context, compression_ratio)
            context_tokens = len(enc.encode(compressed))
            context = compressed
        
        # Now send to LLM
        response = llm.generate(
            system_prompt=SYSTEM_PROMPT,
            context=context,
            query=query
        )
        
        return response
    

**Result:** Stayed under token limits, never wasted tokens on too-large contexts.

# The Results

|Optimization|Before|After|Savings|
|:-|:-|:-|:-|
|Chunk size|1,000 tokens|400 tokens|Smaller chunks|
|Retrieved docs|5 docs|3 docs|40% fewer|
|Context compression|None|60% reduction|2x tokens|
|Query simplification|None|Applied|Better retrieval|
|Context caching|0% hit rate|35% hit rate|35% queries free|
|Token counting|None|Applied|No waste|
|**Total per query**|**5,500 tokens**|**2,200 tokens**|**60% reduction**|

**Cost Impact:**

* Before: 100k queries × 5,500 tokens × $0.03/1K = **$16.50/day** ($495/month)
* After: 100k queries × 2,200 tokens × $0.03/1K = **$6.60/day** ($198/month)
* **Savings: $297/month (60% reduction)**

**Accuracy Impact:**

* Before: 92% accuracy
* After: 92% accuracy (unchanged)

# Important Caveat

These optimizations come with tradeoffs:

1. **Query simplification** adds latency (extra LLM call, even if cheap)
2. **Context compression** could lose edge-case information
3. **Caching** reduces freshness (stale context for 24 hours)
4. **Aggressive filtering** might miss relevant documents

We accepted these tradeoffs. Your situation might differ.

# Implementation Difficulty

* **Easy:** Token counting (1 hour)
* **Easy:** Retrieval filtering (2 hours)
* **Medium:** Query simplification (3 hours)
* **Medium:** Context compression (4 hours)
* **Medium:** Semantic chunking (4 hours)
* **Hard:** Context-level caching (5 hours)

**Total:** \~19 hours of engineering work to save $297/month.

Payback period: \~1 month.

# Code: Complete Pipeline

    class OptimizedRAGPipeline:
        def __init__(self, llm, retriever, cache):
            self.llm = llm
            self.retriever = retriever
            self.cache = cache
            self.encoder = tiktoken.encoding_for_model("gpt-4")
        
        def process_query(self, user_query: str) -&gt; str:
            """Complete optimized pipeline"""
            
            # Step 1: Simplify query
            simplified_query = self.simplify_query(user_query)
            
            # Step 2: Retrieve with caching
            context = self.cached_context_retrieval(simplified_query)
            
            # Step 3: Smart token handling
            response = self.smart_context_sending(
                context=context,
                query=user_query
            )
            
            return response
        
        def simplify_query(self, query: str) -&gt; str:
            """Extract main topic from query"""
            # Implementation from above
            pass
        
        def cached_context_retrieval(self, query: str) -&gt; str:
            """Retrieve and cache at context level"""
            # Implementation from above
            pass
        
        def smart_context_sending(self, context: str, query: str) -&gt; str:
            """Send context with token optimization"""
            # Implementation from above
            pass
    

# Questions for the Community

1. **Are you doing context-level caching?** We found 35% hit rate. What's your experience?
2. **How much quality loss do you see from compression?** We measured \~1-2% accuracy drop.
3. **Query simplification latency trade:** Is it worth the extra LLM call?
4. **Semantic chunking:** Are you doing it? How much better are results?
5. **Token optimization:** What's the best bang-for-buck optimization you've found?

# Edit: Responses

**On query simplification latency:** \~200-300ms added. With caching, only happens once per unique query. Worth it for most systems.

**On context compression quality:** We tested with GPT-3.5-turbo for compression (cheaper). Slightly more loss than GPT-4, but acceptable trade. Saves another $150/month.

**On whether these are general:** Yes, we tested on 3 different domains (legal, technical docs, customer support). Results were similar.

**On LangChain compatibility:** All of this integrates cleanly with LangChain's abstractions. No fighting the framework.

Would love to hear if others have found different optimizations. Token cost is becoming the bottleneck.


Top 5 comments:

Comment 1:
Author: KegOfAppleJuice
Score: 6
Created UTC: 1766916821.0
Comment: Even though it's written up by AI, nice overview of things one can do for token optimisation

Comment 2:
Author: OnyxProyectoUno
Score: 4
Created UTC: 1766963406.0
Comment: The semantic chunking approach is solid but you're still doing fixed-size boundaries. The real gains come from content-aware splitting that preserves logical units.

Your compression step is interesting but risky. Compressing 3000 tokens to 800 means you're losing 70% of the information and trusting an LLM to keep what matters. That works until it doesn't. The accuracy might look the same in aggregate but you're probably missing edge cases where the compressed context drops critical details.

The context-level caching is clever. Most people only cache final responses which misses the opportunity to reuse expensive retrieval work. 35% hit rate is good, though 24-hour TTL might be too aggressive depending on how often your docs change.

One thing that jumps out is you're optimizing downstream when the real waste might be upstream. If your chunks are poorly formed to begin with, you're just optimizing garbage. Bad chunks mean more retrieval attempts, more irrelevant context, more compression losses. Have you looked at what your semantic chunker is actually producing? Sometimes the chunking strategy matters more than all the downstream optimization combined.

The token counting is table stakes at this point. Surprised more people aren't doing that by default.

What's your chunk quality like after the semantic splitting? Are you seeing clean breaks at logical boundaries or is it still cutting mid-thought sometimes?

Comment 3:
Author: cmndr_spanky
Score: 2
Created UTC: 1766961891.0
Comment: Your solution isn’t good

Comment 4:
Author: MathematicianSome289
Score: 1
Created UTC: 1766935484.0
Comment: This is great just don’t need the code

Comment 5:
Author: saintskytower
Score: 1
Created UTC: 1766946786.0
Comment: Moderator Comment

A reminder to everyone participating:

• Personal attacks, profanity, and calls to mass-report users or content violate Rule 2 (“Be nice”) and will be removed.
• If you believe a post is spam or low quality, use the report function. Do not harass other users or moderators publicly.
• Moderation decisions are made by the moderation team based on the posted rules, not by hostile comments.

The post itself remains open for technical discussion. Keep replies focused on the substance of the content and within subreddit rules.